"""
EPUB XHTMLæ–‡ä»¶æ ¼å¼åŒ–å·¥å…·ï¼ˆæ”¯æŒæ–‡ä»¶åæ’åºå¤„ç†ï¼‰
åŠŸèƒ½ï¼š
1. æŒ‰æ–‡ä»¶åé¡ºåºå¤„ç†æ–‡ä»¶ï¼ˆä»å°åˆ°å¤§ï¼‰
2. æ ‡å‡†åŒ–å¤´éƒ¨å£°æ˜ï¼ˆXML + DOCTYPEï¼‰
3. ä¿®å¤è‡ªé—­åˆæ ‡ç­¾å¹¶ä¿æŒæ¢è¡Œï¼ˆç‰¹å®šæ¡ä»¶ä¸‹è·³è¿‡ï¼‰
4. 4ç©ºæ ¼ç¼©è¿›æ ¼å¼åŒ–ï¼ˆç‰¹å®šæ¡ä»¶ä¸‹è·³è¿‡ï¼‰
5. å°†h2-spanæ–‡æœ¬å¤åˆ¶åˆ°titleï¼ˆç‰¹å®šæ¡ä»¶ä¸‹è·³è¿‡ï¼‰
6. æ¸…ç†headéƒ¨åˆ†
7. ç‰¹å®šå·/éƒ¨/ç•ªå¤–æ ‡é¢˜è·³è¿‡bodyæ ¼å¼åŒ–å’Œæ ‡é¢˜æ›´æ–°
"""
import os
import argparse
import re
from lxml import etree

# éœ€è¦ä¿ç•™æ¢è¡Œçš„è‡ªé—­åˆæ ‡ç­¾åˆ—è¡¨
LINE_PRESERVING_TAGS = {"p", "div", "span", "a", "ul", "li", "h1", "h2", "h3", "br"}

# æ ‡å‡†å¤´éƒ¨å£°æ˜
STANDARD_XML_DECLARATION = '<?xml version="1.0" encoding="UTF-8"?>'
STANDARD_DOCTYPE = '<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">'

# éœ€è¦è·³è¿‡bodyæ ¼å¼åŒ–çš„æ ‡é¢˜æ¨¡å¼
SKIP_TITLE_PATTERNS = [
    r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶ç™¾åƒä¸‡\d]+å·\s+.+',  # ç¬¬Xå· XXX
    r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶ç™¾åƒä¸‡\d]+éƒ¨\s+.+',  # ç¬¬Xéƒ¨ XXX
    r'^ç•ªå¤–\s+.+',                      # ç•ªå¤– XXX
    r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶ç™¾åƒä¸‡\d]+å·$',   # ä»…"ç¬¬Xå·"
    r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶ç™¾åƒä¸‡\d]+éƒ¨$',   # ä»…"ç¬¬Xéƒ¨"
    r'^ç•ªå¤–$'                          # ä»…"ç•ªå¤–"
]

def should_skip_body_formatting(title_text):
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦ç¬¦åˆè·³è¿‡bodyæ ¼å¼åŒ–çš„æ¡ä»¶"""
    if not title_text:
        return False
    title_text = title_text.strip()
    return any(re.match(pattern, title_text) for pattern in SKIP_TITLE_PATTERNS)

def extract_title_from_raw_content(content):
    """ç›´æ¥ä»åŸå§‹å†…å®¹ä¸­æå–<title>æ ‡ç­¾å†…å®¹"""
    title_match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
    return title_match.group(1).strip() if title_match else None

def extract_declarations(content):
    """åˆ†ç¦»XMLå£°æ˜å’ŒDOCTYPEå£°æ˜"""
    xml_declaration = ""
    doctype = ""
    body_start = 0

    xml_match = re.search(r'<\?xml[^>]*\?>', content)
    if xml_match:
        body_start = xml_match.end()
        xml_declaration = xml_match.group(0).strip()

    if body_start > 0:
        remaining = content[body_start:]
    else:
        remaining = content

    doctype_match = re.search(r'<!DOCTYPE[^>]+>', remaining, re.IGNORECASE)
    if doctype_match:
        doctype = doctype_match.group(0).strip()
        body_start += doctype_match.end()

    return xml_declaration, doctype, content[body_start:].lstrip()

def format_self_closing_tags(content):
    """é¢„å¤„ç†ï¼šä¿ç•™ç‰¹å®šè‡ªé—­åˆæ ‡ç­¾çš„å•ç‹¬æ¢è¡Œç‰¹æ€§"""
    pattern = r'<([a-z]+:)?([a-z0-9]+)([^>]*)/>'

    def replace_tag(match):
        ns_prefix = match.group(1) or ""
        tag_name = match.group(2)
        attributes = match.group(3)

        if tag_name in LINE_PRESERVING_TAGS:
            start_pos = match.start()
            preceding_text = content[:start_pos]
            if "\n" in preceding_text:
                last_newline = preceding_text.rfind("\n")
                if not preceding_text[last_newline+1:start_pos].strip():
                    return f"<{ns_prefix}{tag_name}{attributes}></{ns_prefix}{tag_name}>"

        return match.group(0)

    return re.sub(pattern, replace_tag, content, flags=re.IGNORECASE)

def fix_self_closing_tags(root):
    """ä¿®å¤éç©ºå…ƒç´ çš„è‡ªé—­åˆæ ‡ç­¾"""
    non_void_elements = {"p", "div", "span", "a", "ul", "li", "h1", "h2", "h3"}

    for tag in root.iter():
        tag_name = tag.tag.split("}")[-1] if '}' in tag.tag else tag.tag
        if tag_name in non_void_elements and tag.text is None and len(tag) == 0:
            tag.text = ""

def standardize_html_tag(content):
    """ç¡®ä¿<html>æ ‡ç­¾åŒ…å«å¿…è¦çš„å±æ€§å’Œå€¼"""
    html_match = re.search(r'<html\b([^>]*)>', content, re.IGNORECASE)
    if not html_match:
        return content

    html_attrs = html_match.group(1)
    attrs_dict = {}

    for attr_match in re.finditer(r'(\w+)\s*=\s*["\']([^"\']*)["\']', html_attrs):
        attrs_dict[attr_match.group(1).lower()] = attr_match.group(2)

    if "xmlns" not in attrs_dict:
        attrs_dict["xmlns"] = "http://www.w3.org/1999/xhtml"
    if "xml:lang" not in attrs_dict:
        attrs_dict["xml:lang"] = "zh-Hans"

    new_attrs = ' '.join([f'{k}="{v}"' for k, v in attrs_dict.items()])
    new_html_tag = f'<html {new_attrs}>'

    return content.replace(html_match.group(0), new_html_tag)

def clean_head_section(root, xhtml_ns):
    """æ¸…ç†headéƒ¨åˆ†ï¼Œåªä¿ç•™titleå’Œstylesheeté“¾æ¥"""
    head = root.find(f".//{{{xhtml_ns}}}head")
    if head is None:
        return

    elements_to_keep = []

    title = head.find(f".//{{{xhtml_ns}}}title")
    if title is not None:
        elements_to_keep.append(title)

    for link in head.findall(f".//{{{xhtml_ns}}}link"):
        rel_attr = link.get("rel", "").lower()
        if "stylesheet" in rel_attr:
            elements_to_keep.append(link)

    head.clear()
    for element in elements_to_keep:
        head.append(element)

def update_title_from_h2(root, xhtml_ns):
    """å°†h2-spanæ–‡æœ¬å¤åˆ¶åˆ°titleæ ‡ç­¾"""
    head = root.find(f".//{{{xhtml_ns}}}head")
    if head is None:
        return

    title = head.find(f".//{{{xhtml_ns}}}title")
    if title is None:
        title = etree.Element(f"{{{xhtml_ns}}}title")
        head.insert(0, title)

    h2 = root.find(f".//{{{xhtml_ns}}}h2")
    if h2 is None:
        return

    span = h2.find(f".//{{{xhtml_ns}}}span")
    if span is not None and span.text:
        title.text = span.text.strip()
    elif h2.text:
        title.text = h2.text.strip()

def format_epub_xhtml_file(input_path, output_path, indent_size=4):
    """æ ¼å¼åŒ–EPUB XHTMLæ–‡ä»¶ï¼ˆç²¾ç¡®æ§åˆ¶ç‰ˆï¼‰"""
    try:
        # è¯»å–åŸå§‹å†…å®¹
        with open(input_path, 'r', encoding='utf-8') as f:
            raw_content = f.read()

        # æå–åŸå§‹æ ‡é¢˜
        raw_title = extract_title_from_raw_content(raw_content)
        skip_body_processing = should_skip_body_formatting(raw_title) if raw_title else False

        if skip_body_processing:
            print(f"â© è·³è¿‡bodyå¤„ç†: {os.path.basename(input_path)} - '{raw_title}'")

        # æ ‡å‡†åŒ–<html>æ ‡ç­¾å±æ€§
        processed_content = standardize_html_tag(raw_content)

        # åˆ†ç¦»å£°æ˜ä¸ä¸»ä½“å†…å®¹
        xml_decl, doctype, body_content = extract_declarations(processed_content)

        # å¼ºåˆ¶ä½¿ç”¨æ ‡å‡†å£°æ˜
        xml_decl = STANDARD_XML_DECLARATION
        doctype = STANDARD_DOCTYPE

        # ==== å…³é”®ä¿®æ”¹ï¼šæ‰€æœ‰æ–‡ä»¶éƒ½è§£ææ–‡æ¡£ ====
        # ä½¿ç”¨ä¸æ”¹å˜ç©ºç™½çš„è§£æå™¨
        parser = etree.XMLParser(
            remove_blank_text=False,
            resolve_entities=False,
            recover=True
        )

        # åˆ›å»ºåŒ…å«å®Œæ•´æ–‡æ¡£çš„å†…å®¹
        full_body = f"<root>{body_content}</root>"  # åŒ…è£¹æ ¹å…ƒç´ ç¡®ä¿è§£ææœ‰æ•ˆ
        root = etree.fromstring(full_body.encode('utf-8'), parser)

        # è·å–å®é™…ä½¿ç”¨çš„å‘½åç©ºé—´
        namespaces = root[0].nsmap if len(root) > 0 else {}
        xhtml_ns = namespaces.get(None, "http://www.w3.org/1999/xhtml")

        # è·å–çœŸæ­£çš„htmlæ ¹å…ƒç´ 
        html_root = root.find(f".//{{{xhtml_ns}}}html")
        if html_root is None:
            html_root = root.find(".//html") or root[0]

        # ==== æ¸…ç†headéƒ¨åˆ†ï¼ˆæ‰€æœ‰æ–‡ä»¶éƒ½æ‰§è¡Œï¼‰ ====
        clean_head_section(html_root, xhtml_ns)

        # ==== æ¡ä»¶æ€§æ›´æ–°æ ‡é¢˜ ====
        if not skip_body_processing:
            update_title_from_h2(html_root, xhtml_ns)

        # ==== æ¡ä»¶æ€§bodyå¤„ç† ====
        if not skip_body_processing:
            # å¤„ç†è‡ªé—­åˆæ ‡ç­¾
            processed_body = format_self_closing_tags(body_content)
            root = etree.fromstring(f"<root>{processed_body}</root>".encode('utf-8'), parser)
            html_root = root.find(f".//{{{xhtml_ns}}}html") or root[0]

            # ä¿®å¤è‡ªé—­åˆæ ‡ç­¾
            fix_self_closing_tags(html_root)

            # åº”ç”¨ç¼©è¿›
            etree.indent(html_root, space=" " * indent_size)

        # åºåˆ—åŒ–ä¸»ä½“
        serialized_body = etree.tostring(
            html_root,
            encoding="utf-8",
            xml_declaration=False,
            pretty_print=not skip_body_processing,
            method="xml"
        ).decode('utf-8')

        # ç§»é™¤åŒ…è£¹çš„æ ¹å…ƒç´ 
        if serialized_body.startswith('<root>'):
            serialized_body = serialized_body[6:-7]

        # åå¤„ç†ï¼šæ¢å¤å•ç‹¬æˆè¡Œçš„ç©ºæ ‡ç­¾
        if not skip_body_processing:
            serialized_body = re.sub(
                r'<(/?)(p|div|span|a|ul|li|h1|h2|h3|br)([^>]*)>\s*</\2>',
                r'<\1\2\3></\2>', 
                serialized_body
            )

        # ç»„åˆæœ€ç»ˆå†…å®¹
        full_content = f"{xml_decl}\n{doctype}\n{serialized_body}"

        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_content)

        return True, None

    except Exception as e:
        return False, f"è§£æå¤±è´¥: {str(e)}"

def main():
    parser = argparse.ArgumentParser(description='EPUB XHTMLæ–‡ä»¶æ‰¹é‡æ ¼å¼åŒ–ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼‰')
    parser.add_argument('source_dir', help='æºæ–‡ä»¶ç›®å½•')
    parser.add_argument('--indent', type=int, default=4, help='ç¼©è¿›ç©ºæ ¼æ•°ï¼ˆé»˜è®¤4ï¼‰')
    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    source_dir = os.path.abspath(args.source_dir)
    base_name = os.path.basename(source_dir)
    parent_dir = os.path.dirname(source_dir)
    output_dir = os.path.join(parent_dir, f"{base_name}_formatted")
    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸ”ğŸ” æ‰«æç›®å½•: {source_dir}")
    print(f"ğŸ“‚ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    errors = []

    # è·å–æ‰€æœ‰XHTMLæ–‡ä»¶
    file_list = []
    for filename in os.listdir(source_dir):
        if filename.lower().endswith(('.xhtml', '.html', '.xml')):
            file_list.append(filename)

    # æŒ‰æ–‡ä»¶åè‡ªç„¶æ’åº
    file_list.sort(key=lambda f: [int(s) if s.isdigit() else s.lower() for s in re.split(r'(\d+)', f)])

    print(f"ğŸ“ŠğŸ“Š æ‰¾åˆ° {len(file_list)} ä¸ªæ–‡ä»¶ï¼ŒæŒ‰æ–‡ä»¶åæ’åºå¤„ç†...")

    for filename in file_list:
        input_file = os.path.join(source_dir, filename)
        output_file = os.path.join(output_dir, filename)

        success, error = format_epub_xhtml_file(input_file, output_file, args.indent)

        if success:
            print(f"âœ… æˆåŠŸ: {filename}")
        else:
            print(f"âŒâŒ å¤±è´¥: {filename} - {error}")
            errors.append(filename)

    print(f"\nğŸ‰ğŸ‰ å®Œæˆï¼å¤„ç†æ–‡ä»¶: {len(file_list) - len(errors)}/{len(file_list)} ä¸ª")
    if errors:
        print(f"âš ï¸ å¤±è´¥æ–‡ä»¶: {', '.join(errors)}")

if __name__ == "__main__":
    main()